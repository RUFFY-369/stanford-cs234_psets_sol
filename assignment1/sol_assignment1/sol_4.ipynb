{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP Value iteration and Policy iteration implementation for the Frozen Lake environment from OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor policy_evaluation, policy_improvement, policy_iteration and value_iteration,\\nthe parameters P, nS, nA, gamma are defined as follows:\\n\\n    P: nested dictionary\\n        From gym.core.Environment\\n        For each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\\n        tuple of the form (probability, nextstate, reward, terminal) where\\n            - probability: float\\n                the probability of transitioning from \"state\" to \"nextstate\" with \"action\"\\n            - nextstate: int\\n                denotes the state we transition to (in range [0, nS - 1])\\n            - reward: int\\n                either 0 or 1, the reward for transitioning from \"state\" to\\n                \"nextstate\" with \"action\"\\n            - terminal: bool\\n              True when \"nextstate\" is a terminal state (hole or goal), False otherwise\\n    nS: int\\n        number of states in the environment\\n    nA: int\\n        number of actions in the environment\\n    gamma: float\\n        Discount factor. Number in range [0, 1)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "\"\"\"\n",
    "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "\n",
    "    P: nested dictionary\n",
    "        From gym.core.Environment\n",
    "        For each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "        tuple of the form (probability, nextstate, reward, terminal) where\n",
    "            - probability: float\n",
    "                the probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "            - nextstate: int\n",
    "                denotes the state we transition to (in range [0, nS - 1])\n",
    "            - reward: int\n",
    "                either 0 or 1, the reward for transitioning from \"state\" to\n",
    "                \"nextstate\" with \"action\"\n",
    "            - terminal: bool\n",
    "              True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "    nS: int\n",
    "        number of states in the environment\n",
    "    nA: int\n",
    "        number of actions in the environment\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    prev_value_function=1e7*np.ones(nS)\n",
    "    \n",
    "    while np.max(np.abs(prev_value_function-value_function)) > tol : \n",
    "        prev_value_function=np.copy(value_function)\n",
    "        \n",
    "        for st in range(nS):\n",
    "            ret=0\n",
    "            #val func for state st\n",
    "            for  prob,nxt_st,reward,_ in P[st][policy[st]]:\n",
    "                ret +=prob*(reward+ gamma*prev_value_function[nxt_st])\n",
    "            value_function[st]=ret\n",
    "    ############################\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "    for s in range(nS):\n",
    "        exp_rew=[]\n",
    "        for ac in range(nA):\n",
    "            ret=0\n",
    "            #val func for state s\n",
    "            for prob,nxt_st,reward,_ in P[s][ac]:\n",
    "                ret+=(prob*(reward+gamma*value_from_policy[nxt_st]))\n",
    "            exp_rew.append(ret)\n",
    "        new_policy[s]=np.argmax(exp_rew)\n",
    "    \n",
    "    ############################\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should call the policy_evaluation() and policy_improvement() methods to\n",
    "    implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "    defined at beginning of file\n",
    "    tol: float\n",
    "        tol parameter used in policy_evaluation()\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    i=0\n",
    "    while True :\n",
    "        i+=1\n",
    "        val_from_policy = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        new_pol=policy_improvement(P, nS, nA, val_from_policy, policy, gamma)\n",
    "        \n",
    "        if np.sum((new_pol-policy))==0:\n",
    "            break\n",
    "        else:\n",
    "            policy=new_pol\n",
    "    value_function=val_from_policy\n",
    "    print(f'Converged in {i} ierations')\n",
    "    ############################\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        Terminate value iteration when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "    prev_value_function=1e7*np.ones(nS)\n",
    "    i=0\n",
    "    while np.max(np.abs(prev_value_function-value_function)) > tol : \n",
    "        i+=1\n",
    "        prev_value_function=np.copy(value_function)\n",
    "        for s in range(nS):\n",
    "            exp_val=[]\n",
    "            \n",
    "            for ac in range(nA):\n",
    "                ret=0\n",
    "                #val func for state s\n",
    "                for prob,nxt_st,reward,_ in P[s][ac]:\n",
    "                    ret+=(prob*(reward+gamma*prev_value_function[nxt_st]))\n",
    "                exp_val.append(ret)\n",
    "            value_function[s]=np.max(exp_val)\n",
    "    print(f'Converged in {i} iterations')\n",
    "     \n",
    "    opt_val=value_function\n",
    "    \n",
    "    for s in range(nS):\n",
    "        exp_rew=[]\n",
    "        \n",
    "        for ac in range(nA):\n",
    "            ret=0\n",
    "            for prob,nxt_st,reward,_ in P[s][ac]:\n",
    "                ret+=(prob*(reward+gamma*opt_val[nxt_st]))\n",
    "            \n",
    "            exp_rew.append(ret)\n",
    "        policy[s]=np.argmax(exp_rew)\n",
    "    \n",
    "    \n",
    "    ############################\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "    \"\"\"\n",
    "\n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        env.render()\n",
    "        time.sleep(0.25)\n",
    "        a = policy[ob]\n",
    "        ob, rew, done, _ = env.step(a)\n",
    "        episode_reward += rew\n",
    "        if done:\n",
    "            break\n",
    "    env.render();\n",
    "    if not done:\n",
    "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "Converged in 6 ierations\n",
      "For policy iteration \n",
      "Optimal Value function : [0.062 0.056 0.07  0.051 0.086 0.    0.11  0.    0.141 0.244 0.297 0.\n",
      " 0.    0.377 0.638 0.   ] \n",
      "Optimal policy : [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
      " \n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "Converged in 27 iterations\n",
      "For value iteration \n",
      "Optimal Value function : [0.063 0.056 0.071 0.052 0.086 0.    0.11  0.    0.141 0.244 0.297 0.\n",
      " 0.    0.378 0.638 0.   ] \n",
      "Optimal policy : [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
      "\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Edit below to run policy and value iteration on different environments and\n",
    "# visualize the resulting policies in action!\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # comment/uncomment these lines to switch between deterministic(4x4/8x8)/stochastic environments\n",
    "    #env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "    #env = gym.make(\"Deterministic-8x8-FrozenLake-v0\")\n",
    "    \n",
    "    env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "    V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    print(f'For policy iteration \\nOptimal Value function : {V_pi} \\nOptimal policy : {p_pi} \\n ')\n",
    "    render_single(env, p_pi, 100)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "    V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    print(f'For value iteration \\nOptimal Value function : {V_vi} \\nOptimal policy : {p_vi} \\n')      \n",
    "    render_single(env, p_vi, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
