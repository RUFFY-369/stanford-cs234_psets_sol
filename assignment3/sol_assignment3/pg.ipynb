{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from config import config\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import scipy.signal\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from utils.general import get_logger, Progbar, export_plot\n",
    "#from config import get_config\n",
    "from OpenGL import GLU\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--env_name', required=True, type=str,\n",
    "                    choices=['cartpole', 'pendulum', 'cheetah'])\n",
    "parser.add_argument('--baseline', dest='use_baseline', action='store_true')\n",
    "parser.add_argument('--no-baseline', dest='use_baseline', action='store_false')\n",
    "parser.set_defaults(use_baseline=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(\n",
    "          mlp_input,\n",
    "          output_size,\n",
    "          scope,\n",
    "          n_layers,\n",
    "          size,\n",
    "          output_activation=None):\n",
    "    \"\"\"\n",
    "  Build a feed forward network (multi-layer perceptron, or mlp)\n",
    "  with 'n_layers' hidden layers, each of size 'size' units.\n",
    "  Use tf.nn.relu nonlinearity between layers.\n",
    "  Args:\n",
    "          mlp_input: the input to the multi-layer perceptron\n",
    "          output_size: the output layer size\n",
    "          scope: the scope of the neural network\n",
    "          n_layers: the number of hidden layers of the network\n",
    "          size: the size of each layer:\n",
    "          output_activation: the activation of output layer\n",
    "  Returns:\n",
    "          The tensor output of the network\n",
    "\n",
    "  TODO: Implement this function. This will be similar to the linear\n",
    "  model you implemented for Assignment 2.\n",
    "  \"tf.layers.dense\" and \"tf.variable_scope\" may be helpful.\n",
    "\n",
    "  A network with n hidden layers has n 'linear transform + nonlinearity'\n",
    "  operations followed by the final linear transform for the output layer\n",
    "  (followed by the output activation, if it is not None).\n",
    "\n",
    "  \"\"\"\n",
    "  #######################################################\n",
    "  #########   YOUR CODE HERE - 7-20 lines.   ############\n",
    "\n",
    "    with tf.variable_scope(scope) as _:\n",
    "        inp=mlp_input\n",
    "        for _ in range(n_layers):\n",
    "            inp=tf.layers.dense(inp,size,tf.nn.relu)\n",
    "        out=tf.layers.dense(inp,output_size, output_activation)\n",
    "\n",
    "  \n",
    "    return out\n",
    "  #######################################################\n",
    "  #########          END YOUR CODE.          ############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG(object):\n",
    "    \"\"\"\n",
    "    Abstract Class for implementing a Policy Gradient Based Algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, env, config, logger=None):\n",
    "        \"\"\"\n",
    "        Initialize Policy Gradient Class\n",
    "\n",
    "        Args:\n",
    "            env: an OpenAI Gym environment\n",
    "            config: class with hyperparameters\n",
    "            logger: logger instance from the logging module\n",
    "\n",
    "        You do not need to implement anything in this function. However,\n",
    "        you will need to use self.discrete, self.observation_dim,\n",
    "        self.action_dim, and self.lr in other methods.\n",
    "\n",
    "        \"\"\"\n",
    "    # directory for training outputs\n",
    "        if not os.path.exists(config.output_path):\n",
    "            os.makedirs(config.output_path)\n",
    "\n",
    "    # store hyperparameters\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        if logger is None:\n",
    "            self.logger = get_logger(config.log_path)\n",
    "        self.env = env\n",
    "\n",
    "    # discrete vs continuous action space\n",
    "        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "\n",
    "        self.lr = self.config.learning_rate\n",
    "\n",
    "    # build model\n",
    "        self.build()\n",
    "    \n",
    "    \n",
    "    def add_placeholders_op(self):\n",
    "        \"\"\"\n",
    "        Add placeholders for observation, action, and advantage:\n",
    "            self.observation_placeholder, type: tf.float32\n",
    "            self.action_placeholder, type: depends on the self.discrete\n",
    "            self.advantage_placeholder, type: tf.float32\n",
    "\n",
    "        HINT: Check self.observation_dim and self.action_dim\n",
    "        HINT: In the case of continuous action space, an action will be specified by\n",
    "        'self.action_dim' float32 numbers (i.e. a vector with size 'self.action_dim')\n",
    "        \"\"\"\n",
    "    #######################################################\n",
    "    #########   YOUR CODE HERE - 8-12 lines.   ############\n",
    "        self.observation_placeholder = tf.placeholder(tf.float32,(None,)+self.env.observation_space.shape,'observation')\n",
    "        if self.discrete:\n",
    "            self.action_placeholder = tf.placeholder(tf.int32,(None,),'action')\n",
    "        else:\n",
    "            self.action_placeholder = tf.placeholder(tf.float32,(None,self.action_dim),'action')\n",
    "\n",
    "    # Define a placeholder for advantages\n",
    "        self.advantage_placeholder = tf.placeholder(tf.float32,(None,),'advantage')\n",
    "    #######################################################\n",
    "    #########          END YOUR CODE.          ############   \n",
    "    \n",
    "    \n",
    "    def build_policy_network_op(self, scope = \"policy_network\"):\n",
    "        \"\"\"\n",
    "        Build the policy network, construct the tensorflow operation to sample\n",
    "        actions from the policy network outputs, and compute the log probabilities\n",
    "        of the actions taken (for computing the loss later). These operations are\n",
    "        stored in self.sampled_action and self.logprob. Must handle both settings\n",
    "        of self.discrete.\n",
    "\n",
    "        Args:\n",
    "                scope: the scope of the neural network\n",
    "\n",
    "        TODO:\n",
    "        Discrete case:\n",
    "            action_logits: the logits for each action\n",
    "                HINT: use build_mlp, check self.config for layer_size and\n",
    "                n_layers\n",
    "            self.sampled_action: sample from these logits\n",
    "                HINT: use tf.multinomial + tf.squeeze\n",
    "            self.logprob: compute the log probabilities of the taken actions\n",
    "                HINT: 1. tf.nn.sparse_softmax_cross_entropy_with_logits computes\n",
    "                         the *negative* log probabilities of labels, given logits.\n",
    "                      2. taken actions are different than sampled actions!\n",
    "\n",
    "        Continuous case:\n",
    "            To build a policy in a continuous action space domain, we will have the\n",
    "            model output the means of each action dimension, and then sample from\n",
    "            a multivariate normal distribution with these means and trainable standard\n",
    "            deviation.\n",
    "\n",
    "            That is, the action a_t ~ N( mu(o_t), sigma)\n",
    "            where mu(o_t) is the network that outputs the means for each action\n",
    "            dimension, and sigma is a trainable variable for the standard deviations.\n",
    "            N here is a multivariate gaussian distribution with the given parameters.\n",
    "\n",
    "            action_means: the predicted means for each action dimension.\n",
    "                HINT: use build_mlp, check self.config for layer_size and\n",
    "                n_layers\n",
    "            log_std: a trainable variable for the log standard deviations.\n",
    "                HINT: think about why we use log std as the trainable variable instead of std\n",
    "                HINT: use tf.get_variables\n",
    "            self.sampled_actions: sample from the gaussian distribution as described above\n",
    "                HINT: use tf.random_normal\n",
    "                HINT: use re-parametrization to obtain N(mu, sigma) from N(0, 1)\n",
    "            self.lobprob: the log probabilities of the taken actions\n",
    "                HINT: use tf.contrib.distributions.MultivariateNormalDiag\n",
    "\n",
    "        \"\"\"\n",
    "    #######################################################\n",
    "    #########   YOUR CODE HERE - 5-10 lines.   ############\n",
    "\n",
    "        if self.discrete:\n",
    "            action_logits = build_mlp(self.observation_placeholder,self.action_dim,scope,\n",
    "                                      self.config.n_layers,\n",
    "                                      self.config.layer_size ,\n",
    "                                      output_activation=self.config.activation)\n",
    "            \n",
    "            self.sampled_action =   tf.reshape(tf.multinomial(action_logits,1),[-1,])# TODO\n",
    "            \n",
    "            ##Missing the negative sign willlead to decrease in avg. reward during training\n",
    "            self.logprob =         - tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.action_placeholder\n",
    "                                                                                   ,logits=action_logits)# TODO\n",
    "        \n",
    "        else:\n",
    "            action_means = build_mlp(self.observation_placeholder,self.action_dim,\n",
    "                                     scope,self.config.n_layers,\n",
    "                                     self.config.layer_size ,\n",
    "                                     output_activation=None) # TODO\n",
    "            \n",
    "            std =              tf.get_variable('log_std',shape=[1,self.action_dim],dtype=tf.float32,\n",
    "                                                   initializer=tf.zeros_initializer, trainable=True)\n",
    "            log_std=tf.tile(tf.log((tf.exp(std) + 1)),[tf.shape(action_means)[0], 1])# TODO\n",
    "            \n",
    "            self.sampled_action =   tf.random_normal((1,),mean=action_means,stddev=log_std)# TODO\n",
    "            \n",
    "            self.dis =          tf.contrib.distributions.MultivariateNormalDiag(action_means,log_std)\n",
    "            \n",
    "            \n",
    "            self.logprob= self.dis.prob(self.action_placeholder)# TODO\n",
    "    #######################################################\n",
    "    #########          END YOUR CODE.          ############\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"\n",
    "        Compute the loss, averaged for a given batch.\n",
    "\n",
    "        Recall the update for REINFORCE with advantage:\n",
    "        θ = θ + α ∇_θ log π_θ(a_t|s_t) A_t\n",
    "        Think about how to express this update as minimizing a\n",
    "        loss (so that tensorflow will do the gradient computations\n",
    "        for you).\n",
    "\n",
    "        You only have to reference fields of 'self' that have already\n",
    "        been set in the previous methods.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    ######################################################\n",
    "    #########   YOUR CODE HERE - 1-2 lines.   ############\n",
    "        self.loss = tf.reduce_mean(-self.logprob*self.advantage_placeholder)# TODO\n",
    "    #######################################################\n",
    "    #########          END YOUR CODE.          ############  \n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_optimizer_op(self):\n",
    "        \"\"\"\n",
    "        Set 'self.train_op' using AdamOptimizer\n",
    "        HINT: Use self.lr, and minimize self.loss\n",
    "        \"\"\"\n",
    "    ######################################################\n",
    "    #########   YOUR CODE HERE - 1-2 lines.   ############\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)# TODO\n",
    "    #######################################################\n",
    "    #########          END YOUR CODE.          ############\n",
    "    \n",
    "    \n",
    "    def add_baseline_op(self, scope = \"baseline\"):\n",
    "        \"\"\"\n",
    "        Build the baseline network within the scope.\n",
    "\n",
    "        In this function we will build the baseline network.\n",
    "        Use build_mlp with the same parameters as the policy network to\n",
    "        get the baseline estimate. You also have to setup a target\n",
    "        placeholder and an update operation so the baseline can be trained.\n",
    "\n",
    "        Args:\n",
    "            scope: the scope of the baseline network\n",
    "\n",
    "        TODO: Set the following fields\n",
    "            self.baseline\n",
    "                HINT: use build_mlp, the network is the same as policy network\n",
    "                check self.config for n_layers and layer_size\n",
    "                HINT: tf.squeeze might be helpful\n",
    "            self.baseline_target_placeholder\n",
    "            self.update_baseline_op\n",
    "                HINT: first construct a loss using tf.losses.mean_squared_error.\n",
    "                HINT: use AdamOptimizer with self.lr\n",
    "\n",
    "        \"\"\"\n",
    "    ######################################################\n",
    "    #########   YOUR CODE HERE - 4-8 lines.   ############\n",
    "        self.baseline = build_mlp(self.observation_placeholder,1,scope,self.config.n_layers,\n",
    "                                                  self.config.layer_size)# TODO\n",
    "        self.baseline_target_placeholder =tf.placeholder(tf.float32,(None,)) # TODO\n",
    "        \n",
    "        loss=tf.losses.mean_squared_error( self.baseline_target_placeholder,tf.squeeze(self.baseline))\n",
    "        self.update_baseline_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(loss)# TODO\n",
    "    #######################################################\n",
    "    #########          END YOUR CODE.          ############\n",
    "    \n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Build the model by adding all necessary variables.\n",
    "\n",
    "        You don't have to change anything here - we are just calling\n",
    "        all the operations you already defined above to build the tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "    # add placeholders\n",
    "        self.add_placeholders_op()\n",
    "    # create policy net\n",
    "        self.build_policy_network_op()\n",
    "    # add square loss\n",
    "        self.add_loss_op()\n",
    "    # add optmizer for the main networks\n",
    "        self.add_optimizer_op()\n",
    "\n",
    "    # add baseline\n",
    "        if self.config.use_baseline:\n",
    "            self.add_baseline_op()\n",
    "            \n",
    "            \n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Assumes the graph has been constructed (have called self.build())\n",
    "        Creates a tf Session and run initializer of variables\n",
    "\n",
    "        You don't have to change or use anything here.\n",
    "        \"\"\"\n",
    "    # create tf session\n",
    "        self.sess = tf.Session()\n",
    "    # tensorboard stuff\n",
    "        self.add_summary()\n",
    "    # initiliaze all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        \n",
    "    def add_summary(self):\n",
    "        \"\"\"\n",
    "        Tensorboard stuff.\n",
    "\n",
    "        You don't have to change or use anything here.\n",
    "        \"\"\"\n",
    "    # extra placeholders to log stuff from python\n",
    "        self.avg_reward_placeholder = tf.placeholder(tf.float32, shape=(), name=\"avg_reward\")\n",
    "        self.max_reward_placeholder = tf.placeholder(tf.float32, shape=(), name=\"max_reward\")\n",
    "        self.std_reward_placeholder = tf.placeholder(tf.float32, shape=(), name=\"std_reward\")\n",
    "\n",
    "        self.eval_reward_placeholder = tf.placeholder(tf.float32, shape=(), name=\"eval_reward\")\n",
    "\n",
    "    # extra summaries from python -> placeholders\n",
    "        tf.summary.scalar(\"Avg Reward\", self.avg_reward_placeholder)\n",
    "        tf.summary.scalar(\"Max Reward\", self.max_reward_placeholder)\n",
    "        tf.summary.scalar(\"Std Reward\", self.std_reward_placeholder)\n",
    "        tf.summary.scalar(\"Eval Reward\", self.eval_reward_placeholder)\n",
    "\n",
    "    # logging\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.file_writer = tf.summary.FileWriter(self.config.output_path,self.sess.graph)\n",
    "        \n",
    "        \n",
    "    def init_averages(self):\n",
    "        \"\"\"\n",
    "        Defines extra attributes for tensorboard.\n",
    "\n",
    "        You don't have to change or use anything here.\n",
    "        \"\"\"\n",
    "        self.avg_reward = 0.\n",
    "        self.max_reward = 0.\n",
    "        self.std_reward = 0.\n",
    "        self.eval_reward = 0.\n",
    "\n",
    "    def update_averages(self, rewards, scores_eval):\n",
    "        \"\"\"\n",
    "        Update the averages.\n",
    "\n",
    "        You don't have to change or use anything here.\n",
    "\n",
    "        Args:\n",
    "            rewards: deque\n",
    "            scores_eval: list\n",
    "        \"\"\"\n",
    "        self.avg_reward = np.mean(rewards)\n",
    "        self.max_reward = np.max(rewards)\n",
    "        self.std_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "\n",
    "        if len(scores_eval) > 0:\n",
    "            self.eval_reward = scores_eval[-1]   \n",
    "            \n",
    "            \n",
    "            \n",
    "    def record_summary(self, t):\n",
    "        \"\"\"\n",
    "        Add summary to tensorboard\n",
    "\n",
    "        You don't have to change or use anything here.\n",
    "        \"\"\"\n",
    "\n",
    "        fd = {\n",
    "          self.avg_reward_placeholder: self.avg_reward,\n",
    "          self.max_reward_placeholder: self.max_reward,\n",
    "          self.std_reward_placeholder: self.std_reward,\n",
    "          self.eval_reward_placeholder: self.eval_reward,\n",
    "        }\n",
    "        summary = self.sess.run(self.merged, feed_dict=fd)\n",
    "    # tensorboard stuff\n",
    "        self.file_writer.add_summary(summary, t) \n",
    "        \n",
    "        \n",
    "\n",
    "    def sample_path(self, env, num_episodes = None):\n",
    "        \"\"\"\n",
    "        Sample paths (trajectories) from the environment.\n",
    "\n",
    "        Args:\n",
    "            num_episodes: the number of episodes to be sampled\n",
    "                if none, sample one batch (size indicated by config file)\n",
    "            env: open AI Gym envinronment\n",
    "\n",
    "        Returns:\n",
    "            paths: a list of paths. Each path in paths is a dictionary with\n",
    "                path[\"observation\"] a numpy array of ordered observations in the path\n",
    "                path[\"actions\"] a numpy array of the corresponding actions in the path\n",
    "                path[\"reward\"] a numpy array of the corresponding rewards in the path\n",
    "            total_rewards: the sum of all rewards encountered during this \"path\"\n",
    "\n",
    "        You do not have to implement anything in this function, but you will need to\n",
    "        understand what it returns, and it is worthwhile to look over the code\n",
    "        just so you understand how we are taking actions in the environment\n",
    "        and generating batches to train on.\n",
    "        \"\"\"\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "\n",
    "        while (num_episodes or t < self.config.batch_size):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.config.max_ep_len):\n",
    "                states.append(state)\n",
    "                done=False\n",
    "                action=0\n",
    "                reward=0\n",
    "                action = self.sess.run(self.sampled_action, feed_dict={self.observation_placeholder : states[-1][None]})[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "                if (done or step == self.config.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.config.batch_size:\n",
    "                    break\n",
    "\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                      \"reward\" : np.array(rewards),\n",
    "                      \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards\n",
    "    \n",
    "    \n",
    "    def get_returns(self, paths):\n",
    "        \"\"\"\n",
    "        Calculate the returns G_t for each timestep\n",
    "\n",
    "        Args:\n",
    "                paths: recorded sample paths.  See sample_path() for details.\n",
    "\n",
    "        Return:\n",
    "                returns: return G_t for each timestep\n",
    "\n",
    "        After acting in the environment, we record the observations, actions, and\n",
    "        rewards. To get the advantages that we need for the policy update, we have\n",
    "        to convert the rewards into returns, G_t, which are themselves an estimate\n",
    "        of Q^π (s_t, a_t):\n",
    "\n",
    "           G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ... + γ^{T-t} r_T\n",
    "\n",
    "        where T is the last timestep of the episode.\n",
    "\n",
    "        TODO: compute and return G_t for each timestep. Use self.config.gamma.\n",
    "        \"\"\"\n",
    "\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "      #######################################################\n",
    "      #########   YOUR CODE HERE - 5-10 lines.   ############\n",
    "            \n",
    "              \n",
    "            returns = []# TODO\n",
    "            len_ep = len(rewards)\n",
    "            for t in range(len_ep):\n",
    "                len_tl = len_ep-t\n",
    "                gamma = np.logspace(0,len_tl, num=len_tl,\n",
    "                                     base=self.config.gamma, endpoint=False)\n",
    "                ret_t = rewards[t:]*gamma\n",
    "                ret_t = ret_t.sum()\n",
    "                returns.append(ret_t)\n",
    "      #######################################################\n",
    "      #########          END YOUR CODE.          ############\n",
    "            all_returns.append(returns)\n",
    "        returns = np.concatenate(all_returns)\n",
    "\n",
    "        return returns\n",
    "    \n",
    "    \n",
    "    def calculate_advantage(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Calculate the advantage\n",
    "\n",
    "        Args:\n",
    "                returns: all discounted future returns for each step\n",
    "                observations: observations\n",
    "        Returns:\n",
    "                adv: Advantage\n",
    "\n",
    "        Calculate the advantages, using baseline adjustment if necessary,\n",
    "        and normalizing the advantages if necessary.\n",
    "        If neither of these options are True, just return returns.\n",
    "\n",
    "        TODO:\n",
    "        If config.use_baseline = False and config.normalize_advantage = False,\n",
    "        then the \"advantage\" is just going to be the returns (and not actually\n",
    "        an advantage).\n",
    "\n",
    "        if config.use_baseline, then we need to evaluate the baseline and subtract\n",
    "          it from the returns to get the advantage.\n",
    "          HINT: evaluate the self.baseline with self.sess.run(...)\n",
    "\n",
    "        if config.normalize_advantage:\n",
    "          after doing the above, normalize the advantages so that they have a mean of 0\n",
    "          and standard deviation of 1.\n",
    "        \"\"\"\n",
    "        adv = returns\n",
    "    #######################################################\n",
    "    #########   YOUR CODE HERE - 5-10 lines.   ############\n",
    "        if self.config.use_baseline:\n",
    "            adv-=self.sess.run(self.baseline,feed_dict={self.observation_placeholder :observations}).squeeze()        # TODO\n",
    "        \n",
    "        if self.config.normalize_advantage:\n",
    "            ep=1e-12  #to avoid overflowing to inf\n",
    "            \n",
    "            adv=(adv-adv.mean())/(adv.std()+ep)    # TODO\n",
    "   \n",
    "    #######################################################\n",
    "    #########          END YOUR CODE.          ############\n",
    "        return adv \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def update_baseline(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Update the baseline from given returns and observation.\n",
    "\n",
    "        Args:\n",
    "                returns: Returns from get_returns\n",
    "                observations: observations\n",
    "        TODO:\n",
    "          apply the baseline update op with the observations and the returns.\n",
    "          HINT: Run self.update_baseline_op with self.sess.run(...)\n",
    "        \"\"\"\n",
    "    #######################################################\n",
    "    #########   YOUR CODE HERE - 1-5 lines.   ############\n",
    "        self.sess.run(self.update_baseline_op,feed_dict={self.observation_placeholder: observations,\n",
    "                                                         self.baseline_target_placeholder: returns}) # TODO\n",
    "    #######################################################\n",
    "    #########          END YOUR CODE.          ############\n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Performs training\n",
    "\n",
    "        You do not have to change or use anything here, but take a look\n",
    "        to see how all the code you've written fits together!\n",
    "        \"\"\"\n",
    "        last_eval = 0\n",
    "        last_record = 0\n",
    "        scores_eval = []\n",
    "\n",
    "        self.init_averages()\n",
    "        scores_eval = [] # list of scores computed at iteration time\n",
    "\n",
    "        for t in range(self.config.num_batches):\n",
    "\n",
    "      # collect a minibatch of samples\n",
    "            paths, total_rewards = self.sample_path(self.env)\n",
    "            scores_eval = scores_eval + total_rewards\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "      # compute Q-val estimates (discounted future returns) for each time step\n",
    "            returns = self.get_returns(paths)\n",
    "            advantages = self.calculate_advantage(returns, observations)\n",
    "\n",
    "      # run training operations\n",
    "            if self.config.use_baseline:\n",
    "                self.update_baseline(returns, observations)\n",
    "            self.sess.run(self.train_op, feed_dict={\n",
    "                    self.observation_placeholder : observations,\n",
    "                    self.action_placeholder : actions,\n",
    "                    self.advantage_placeholder : advantages})\n",
    "\n",
    "      # tf stuff\n",
    "            if (t % self.config.summary_freq == 0):\n",
    "                self.update_averages(total_rewards, scores_eval)\n",
    "                self.record_summary(t)\n",
    "\n",
    "      # compute reward statistics for this batch and log\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            sigma_reward = np.sqrt(np.var(total_rewards) / len(total_rewards))\n",
    "            msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "            self.logger.info(msg)\n",
    "\n",
    "            if  self.config.record and (last_record > self.config.record_freq):\n",
    "                self.logger.info(\"Recording...\")\n",
    "                last_record =0\n",
    "                self.record()\n",
    "\n",
    "        self.logger.info(\"- Training done.\")\n",
    "        export_plot(scores_eval, \"Score\", config.env_name, self.config.plot_output)\n",
    "        \n",
    "        \n",
    "    def evaluate(self, env=None, num_episodes=1):\n",
    "        \"\"\"\n",
    "        Evaluates the return for num_episodes episodes.\n",
    "        Not used right now, all evaluation statistics are computed during training\n",
    "        episodes.\n",
    "        \"\"\"\n",
    "        if env==None: env = self.env\n",
    "        paths, rewards = self.sample_path(env, num_episodes)\n",
    "        avg_reward = np.mean(rewards)\n",
    "        sigma_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "        msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "        self.logger.info(msg)\n",
    "        return avg_reward \n",
    "    \n",
    "    def record(self):\n",
    "        \"\"\"\n",
    "        Recreate an env and record a video for one episode\n",
    "        \"\"\"\n",
    "        env = gym.make(self.config.env_name)\n",
    "        env = gym.wrappers.Monitor(env, self.config.record_path, video_callable=lambda x: True, resume=True)\n",
    "        self.evaluate(env, 1)\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Apply procedures of training for a PG.\n",
    "        \"\"\"\n",
    "    # initialize\n",
    "        self.initialize()\n",
    "    # record one game at the beginning\n",
    "        if self.config.record:\n",
    "            self.record()\n",
    "    # model\n",
    "        self.train()\n",
    "    # record one game at the end\n",
    "        if self.config.record:\n",
    "            self.record()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #args = parser.parse_args()\n",
    "    #config = get_config(args.env_name, args.use_baseline)\n",
    "    env = gym.make(config.env_name)\n",
    "    # train model\n",
    "    model = PG(env, config)\n",
    "    model.run()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In https://github.com/openai/gym-http-api/issues/2, we 97             # discovered that someone using Xmonad on Arch was having AttributeError: 'ImageData' object has no attribute 'data'\n",
    "\n",
    "................In case of the above error try downgrading 'pyglet' installed in the environment to 'pyglet 1.3.2'..................\n",
    "\n",
    "\n",
    "\n",
    "And this github issue might help for 'HalfCheetah-v1' simulation mujoco error on 'Windows OS'\n",
    ":>https://github.com/openai/mujoco-py/issues/223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rough check of operations\n",
    "(None,)+(3,3) #As tuples are immutable so this will lead to a new tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
