{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "from utils.general import get_logger\n",
    "from utils.test_env import EnvTest\n",
    "from core.deep_q_learning import DQN\n",
    "from q1_schedule import LinearExploration, LinearSchedule\n",
    "\n",
    "from configs.q2_linear import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Linear(DQN):\n",
    "    \"\"\"\n",
    "    Implement Fully Connected with Tensorflow\n",
    "    \"\"\"\n",
    "    def add_placeholders_op(self):\n",
    "        \"\"\"\n",
    "        Adds placeholders to the graph\n",
    "\n",
    "        These placeholders are used as inputs to the rest of the model and will be fed\n",
    "        data during training.\n",
    "        \"\"\"\n",
    "        # this information might be useful\n",
    "        state_shape = list(self.env.observation_space.shape)\n",
    "        img_height,img_width,nchannels=state_shape[0],state_shape[1],state_shape[2]\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            Add placeholders:\n",
    "            Remember that we stack 4 consecutive frames together.\n",
    "                - self.s: batch of states, type = uint8\n",
    "                    shape = (batch_size, img height, img width, nchannels x config.state_history)\n",
    "                - self.a: batch of actions, type = int32\n",
    "                    shape = (batch_size)\n",
    "                - self.r: batch of rewards, type = float32\n",
    "                    shape = (batch_size)\n",
    "                - self.sp: batch of next states, type = uint8\n",
    "                    shape = (batch_size, img height, img width, nchannels x config.state_history)\n",
    "                - self.done_mask: batch of done, type = bool\n",
    "                    shape = (batch_size)\n",
    "                - self.lr: learning rate, type = float32\n",
    "        \n",
    "        (Don't change the variable names!)\n",
    "        \n",
    "        HINT: \n",
    "            Variables from config are accessible with self.config.variable_name.\n",
    "            Check the use of None in the dimension for tensorflow placeholders.\n",
    "            You can also use the state_shape computed above.\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        ################YOUR CODE HERE (6-15 lines) ##################\n",
    "\n",
    "        self.s=tf.placeholder(tf.uint8,(None,img_height,img_width,nchannels*self.config.state_history),'state')\n",
    "        self.a=tf.placeholder(tf.int32,(None),'action')\n",
    "        self.r=tf.placeholder(tf.float32,(None),'reward')\n",
    "        self.sp=tf.placeholder(tf.uint8,(None,img_height,img_width,nchannels*self.config.state_history),'next_state')\n",
    "        self.done_mask=tf.placeholder(tf.bool,(None),'done_mask')\n",
    "        self.lr=tf.placeholder(dtype=tf.float32,name='lr')\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n",
    "\n",
    "\n",
    "    def get_q_values_op(self, state, scope, reuse=False):\n",
    "        \"\"\"\n",
    "        Returns Q values for all actions\n",
    "\n",
    "        Args:\n",
    "            state: (tf tensor) \n",
    "                shape = (batch_size, img height, img width, nchannels x config.state_history)\n",
    "            scope: (string) scope name, that specifies if target network or not\n",
    "            reuse: (bool) reuse of variables in the scope\n",
    "\n",
    "        Returns:\n",
    "            out: (tf tensor) of shape = (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        # this information might be useful\n",
    "        num_actions = self.env.action_space.n\n",
    "\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            Implement a fully connected with no hidden layer (linear\n",
    "            approximation with bias) using tensorflow.\n",
    "\n",
    "        HINT: \n",
    "            - You may find the following functions useful:\n",
    "                - tf.layers.flatten\n",
    "                - tf.layers.dense\n",
    "\n",
    "            - Make sure to also specify the scope and reuse\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        ################ YOUR CODE HERE - 2-3 lines ################## \n",
    "        inp=layers.flatten(state,scope=scope)\n",
    "        out=layers.fully_connected(inp,num_actions,reuse=reuse,scope=scope,activation_fn=None)\n",
    "\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def add_update_target_op(self, q_scope, target_q_scope):\n",
    "        \"\"\"\n",
    "        update_target_op will be called periodically \n",
    "        to copy Q network weights to target Q network\n",
    "\n",
    "        Remember that in DQN, we maintain two identical Q networks with\n",
    "        2 different sets of weights. In tensorflow, we distinguish them\n",
    "        with two different scopes. If you're not familiar with the scope mechanism\n",
    "        in tensorflow, read the docs\n",
    "        https://www.tensorflow.org/programmers_guide/variable_scope\n",
    "\n",
    "        Periodically, we need to update all the weights of the Q network \n",
    "        and assign them with the values from the regular network. \n",
    "        Args:\n",
    "            q_scope: (string) name of the scope of variables for q\n",
    "            target_q_scope: (string) name of the scope of variables\n",
    "                        for the target network\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            Add an operator self.update_target_op that for each variable in\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES that is in q_scope, assigns its\n",
    "            value to the corresponding variable in target_q_scope\n",
    "\n",
    "        HINT: \n",
    "            You may find the following functions useful:\n",
    "                - tf.get_collection\n",
    "                - tf.assign\n",
    "                - tf.group (the * operator can be used to unpack a list)\n",
    "\n",
    "        (be sure that you set self.update_target_op)\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        ################### YOUR CODE HERE - 5-10 lines #############\n",
    "        \n",
    "        qscope_coll=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,q_scope)\n",
    "        tqscope_coll=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,target_q_scope)\n",
    "        assgn=[tf.assign(tqscope_coll[i],qscope_coll[i]) for i in range(len(qscope_coll))]\n",
    "        self.update_target_op=tf.group(*assgn)\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n",
    "\n",
    "\n",
    "    def add_loss_op(self, q, target_q):\n",
    "        \"\"\"\n",
    "        Sets the loss of a batch, self.loss is a scalar\n",
    "\n",
    "        Args:\n",
    "            q: (tf tensor) shape = (batch_size, num_actions)\n",
    "            target_q: (tf tensor) shape = (batch_size, num_actions)\n",
    "        \"\"\"\n",
    "        # you may need this variable\n",
    "        num_actions = self.env.action_space.n\n",
    "\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            The loss for an example is defined as:\n",
    "                Q_samp(s) = r if done\n",
    "                          = r + gamma * max_a' Q_target(s', a')\n",
    "                loss = (Q_samp(s) - Q(s, a))^2 \n",
    "        HINT: \n",
    "            - Config variables are accessible through self.config\n",
    "            - You can access placeholders like self.a (for actions)\n",
    "                self.r (rewards) or self.done_mask for instance\n",
    "            - You may find the following functions useful\n",
    "                - tf.cast\n",
    "                - tf.reduce_max\n",
    "                - tf.reduce_sum\n",
    "                - tf.one_hot\n",
    "                - tf.squared_difference\n",
    "                - tf.reduce_mean\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        ##################### YOUR CODE HERE - 4-5 lines #############\n",
    "        q_samp=tf.where(self.done_mask,self.r,(self.r+ (self.config.gamma*tf.reduce_max(target_q,axis=1))))\n",
    "        sum_ind=tf.one_hot(self.a,num_actions)\n",
    "        q_sa=tf.reduce_sum(q*sum_ind,axis=1)\n",
    "        self.loss=tf.reduce_mean(tf.squared_difference(q_samp,q_sa))\n",
    "        \n",
    "\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n",
    "\n",
    "\n",
    "    def add_optimizer_op(self, scope):\n",
    "        \"\"\"\n",
    "        Set self.train_op and self.grad_norm\n",
    "        Args:\n",
    "            scope: (string) scope name, that specifies if target network or not\n",
    "        \"\"\"\n",
    "\n",
    "        ##############################################################\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. get Adam Optimizer\n",
    "            2. compute grads with respect to variables in scope for self.loss\n",
    "            3. if self.config.grad_clip is True, then clip the grads\n",
    "                by norm using self.config.clip_val \n",
    "            4. apply the gradients and store the train op in self.train_op\n",
    "                (sess.run(train_op) must update the variables)\n",
    "            5. compute the global norm of the gradients (which are not None) and store \n",
    "                this scalar in self.grad_norm\n",
    "\n",
    "        HINT: you may find the following functions useful\n",
    "            - tf.get_collection\n",
    "            - optimizer.compute_gradients\n",
    "            - tf.clip_by_norm\n",
    "            - optimizer.apply_gradients\n",
    "            - tf.global_norm\n",
    "             \n",
    "             you can access config variables by writing self.config.variable_name\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        #################### YOUR CODE HERE - 8-12 lines #############\n",
    "\n",
    "        opt=tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        trvar_scope=tf.get_collection( tf.GraphKeys.TRAINABLE_VARIABLES,scope=scope)\n",
    "        grads=opt.compute_gradients(self.loss,trvar_scope)\n",
    "        if self.config.grad_clip :\n",
    "            grads=[(tf.clip_by_norm(grad[0],self.config.clip_val),grad[1]) for grad in grads] \n",
    "        self.train_op=opt.apply_gradients(grads)\n",
    "        self.grad_norm=tf.global_norm([grad[0] for grad in grads])\n",
    "\n",
    "\n",
    "        ##############################################################\n",
    "        ######################## END YOUR CODE #######################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = EnvTest((5, 5, 1))\n",
    "\n",
    "    # exploration strategy\n",
    "    exp_schedule = LinearExploration(env, config.eps_begin, \n",
    "            config.eps_end, config.eps_nsteps)\n",
    "\n",
    "    # learning rate schedule\n",
    "    lr_schedule  = LinearSchedule(config.lr_begin, config.lr_end,\n",
    "            config.lr_nsteps)\n",
    "\n",
    "    # train model\n",
    "    model = Linear(env, config)\n",
    "    model.run(exp_schedule, lr_schedule)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
